\chapter{Introduction}




\section{Basics of Provenance Systems}
Provenance is simply data quality. Provenance is the relationship among all elements that contributed to the existence of a piece of data.The provenance of data focuses on the history of changes and movement of data. The history of data changes can include subsetting, formatting, semantic transformation, syntactic transformation and ingesting new data to the existing data. To maintain or proved the quality of data the lineage of the data needs to collected throughout the data transformational process. The metadata to ensure the quality of the data is to be generated in systematic function. All the information about the elements and the relationship among all the elements (sources, processing steps, contextual information and dependencies) should include in the definition of provenance capturing. 


There are clear differences between process provenance (focusing on workflow execution and the execution environment) and data provenance (focusing on creation and transformation of data). Understanding the type of provenance information of interest and how it will be used can help inform the decision for how to capture the provenance information. The provenance of data can often be collected automatically by saving system logs for events, system inputs, and system outputs but this kind of the provenance does not capture the insight and the rational decision made by the system. 

In practical situations, it is not easy to completely capture all types of provenance information. Provenance information can be captured with varying levels of detail.  Provenance granularity describes the level of detail of the captured information. The level of provenance granularity is motivated by the use of the particular system. In this project, we are focused towards the data provenance mechanism and technique used to capture in a distributed setup.

\subsection{Purpose}
Capturing the process of formation of data in the IoT ecosystem is like three-folding the already data being generated by the sensors. So, there must a concrete reason for doing this. One simple answer to this is that the data in possession will support the ongoing analysis or some-what enhances the result once the results are completed.

Recall of the analytic process is one of the common purposes to capture provenance. Recall enables awareness and understanding of what processing went through and what task are pending. Recalling is important for analytic clarity and efficiency especially when the processing step is complex and interconnected to another component in the architecture. 

Another common purpose of provenance information is to reproduce previously obtain results. This help to verify that the result is accurate and can be trusted. Difference between the results after toggling environment and context parameter can be clearly seen resulting in choosing the best suitable parameter for the upcoming deployment.

Another purpose for provenance involves reviewing the processing step itself and the environment it was run on. This Meta-analysis of processes makes it possible to review and evaluate the step and decision made by the component for the creation of the data. Meta-analysis help to extract patterns and help to optimize performance and suggestion on the changes in real time if need be.

\subsection{Provenance in IoT}
Data in IoT ecosystem is produced in large velocity, volume, and variety. In order to examine and maintain the correctness of the data, provenance data systems are introduced to increase the authenticity. Due to the complicated structure of IoT pipeline in which data comes from distributed nodes across the architecture extremely hard to track manually provenance system is necessary for any sort of insurance on the existing data properties.

Numerous jobs applying complex operation on them are performed which is then propagated to produce some sort of insight from the data or maybe feed into a machine learning algorithm. Finding the reason for an anomaly or outlier from the huge chunk for the data scientist resulting in unexpected results can be a daunting task without capturing of provenance information. 

Debugging unexpected results can be narrow down to the components through which the data has gone through saving a lot of time. This help makes analytic decision which was unable to make without the help of the lineage of the data captured. In case of a distributed system where the data is digitally transformed and derived from numerous sources by applying complex function in various context can produce different results. The advantage of using provenance system with the existing system gives the ability to use the data produced by giving the user transparent view of the whole process and the underlying mechanism used to collect it across a different component of a data-processing workflow so the user knows how the data is molded before it reached to the endpoint.

Capturing provenance information in a distributed system gives insight not only to the data-dependencies but also for fault tolerance and usage statistics. Collection of provenance information is useful in many contexts, such as verifying result and explaining the existing of the item. Capturing provenance information for any specific workflow or process make the user of the data to easily follow the origin of the data. 


\subsection{Challenges}
Collecting provenance for any given system is a difficult task at hand and requires a deep understanding of the underlying architecture of the system to implement an efficient and useful provenance system. This includes taking into account that every architectural design in every situation is motivated by some core values behind that system which need to be taken into account when building a provenance system which tracks each and every movement of the system overall. There is a lot unanswered question provenance architect has to decide on before building a model which suit the system and fulfill the sole purpose it is built for. Typically, when talking about provenance in IoT domain where the addition of one component can scale the data generated exponential can cause disastrous effect if the decision is not made after suitable consideration and trade-offs in mind. 

Provenance collecting for this kind of system must be able to scale to both large volumes of data and numerous operation to avoid being a bottleneck. Distributed pipeline are difficult to manage and control especially when there is a great chance of failure of the system and avoid running the provenance system when some component is not working correctly. This might corrupt the data and will produce gibberish data not useful for anyone.

Provenance system should consider the finer granularity in which it captures the transformation of data. Some component of the system is transparent in term of how the data is manipulated.  On the other hand, the black-box operator is not transparent enough and does not know how the transformation is taking place. Producing highly accurate provenance for this kind of operation requires specific techniques which incur time overhead to capture and there is a direct trade-off between the performance of the system.



\section{Related Work}




\section{Project Organization}

The project was started as part of the Cloud Prototyping course of TU Berlin in October 2017. 
With a team of 7 students that did not know each other before, it was necessary to organize the distribution of tasks and use of tools internally. 
This organization was not set beforehand, but evolved continuously over the course of this semester.

\subsection{Task distribution}

When starting this project as a team of equal students, the distribution of tasks was not trivial as we first had to come up with a problem, use-case and a general vision of what we wanted to achieve. 
Therefore initially everybody had to figure that out for themselves, before discussing it in the group and coming to an agreement.

After making these high-level decisions, tasked were started to be distributed based on personal preferences. 
The granularity of tasks was evolving over time, though quite differently for different tasks.
For example, it became clear at the beginning that we needed a simulation of a smart grid on which we could test our prototype, which resulted in rather precise implementation tasks early on. 
However, the design of the data model took more research and was also revised repeatedly, so that the first fine grained implementation tasks were started some weeks later.

After the first five weeks a scrumboard was introduced \cite{zenhub}. 
Before that, regular github issues were used to define task distribution. 
However, with ever more fine grained and interconnected implementation tasks, it became harder to keep the overview. 
The scrumboard therefore helped to keep a better overview and also subjectively increased the teams performance, especially once we started to set deadlines on a weekly basis.

The Following table \ref{table:responsibilities} was set before the midterm presentation to define responsibilities, especially when it comes to questions regarding the components from within the team as well as from outsiders.


\renewcommand{\arraystretch}{1.4}
\begin{center}
 \begin{tabular}{| m{18em} m{10em} |} 
 \hline
 Responsibility & Name  \\
 \hline\hline
 IoT Pipeline & Kevin \\ 
 \hline
Data Model + Provenance Collector & Mukrram \\
 \hline
 Backend & Vinoth, Mukrram \\
 \hline
 Databases & Talal \\
 \hline
 Frontend & Darshan \\ 
 \hline
 Deployment \& Testing & Gerrit \\
 \hline
Project Management & Ron \\
 \hline
\end{tabular}
\captionof{table}{Distribution of Responsibilities}
\label{table:responsibilities}
\end{center}

\subsection{Tools}

In order to work effectively and potentially simultaneously as a team of 7 members, the use of modern organization tools was necessary. 
The use of GitHub issues and the scrumboard was mentioned already above. 
The use of Slack for internal communication and GitHub for code repositories is further explained below. 
Further tools that aided in code development such Travis CI for continuous integration can be found in the implementation chapter.

\subsubsection{Slack}

Slack had been chosen as the main tool for communication, since it is a very commonly used tool for team organization and all team members were already familiar with it. 

Initially only a "general" channel for discussions and a "links" channel for an overview of important links were created.
Next integrations for GitHub were added, which allowed to track commits and issues for each repository in a separate channel.
Also over time, additional channels were created for more specific tasks, that allowed to keep conversations about different topics (e.g. organization of meetings vs. implementation issues with a certain component) separate, leading to a better overview and less "noise" for members not involved in a specific issue.

\subsubsection{GitHub}

GitHub was chosen as the tool for distributed version control for our coding efforts, again because of the familiarity of all team members.
Different components of our software project were developed in different repositories, in order to keep a better overview and establish clear boundaries.

The first repository that was created was \emph{Krymnos/IDP} which functioned as the main repository for GitHub issues and for the first component that was developed: the pipeline implementation needed for deploying and testing our system.
Next the repository for the provenance daemon and API was created.
Then the repositories for the Backend and Frontend services were created and lastly a repository for archiving the tools and setups we used to facilitate our benchmarks.
The links to these repositories are listed in the order they were mentioned below. Details on the components that were developed and the benchmarks conducted can be found in Chapter 2 and 3 respectively.

\begin{itemize}
  \item https://github.com/Krymnos/IDP
  \item https://github.com/Krymnos/Provenance-System-for-IoT
  \item https://github.com/Krymnos/IDP-backend
  \item https://github.com/Krymnos/IDP-frontend
  \item https://github.com/Krymnos/idp-benchmark
\end{itemize}

\subsection{This Report}

This final report was written by all members of the team. 
The distribution of writing tasks was again done by personal preferences. 
The Following table \ref{table:authors} lists the authors of each section.
For a more detailed view on contributions to this report, please refer to the commit history available on our GitHub repository\footnote{\url{https://github.com/Krymnos/IDP/tree/master/report}}, where the Latex code for this report was managed.

\begin{table}
\begin{center}
 \begin{tabular}{| m{18em} m{10em} |} 
 \hline
1. Introduction                                                   &                    \\
 \hline
* Basics of Provenance Systems                                    &     Talal               \\
 \hline
* Related Work                                                    &          Mukrram          \\
 \hline
* Project Organization                                            &         Ron           \\
 \hline
* Use Case                                                        &   Ron                 \\
 \hline
2. Approach / Implementation                                      &                    \\
 \hline
* Implementation Overview/Approach                                &    Mukrram                \\
 \hline
* Data Model                                                      &        Mukrram            \\
 \hline
* Pipeline Implementation                                         &      Kevin              \\
 \hline
* Provenance API                                                     &         Vinoth           \\
 \hline
* Cassandra as Provenance DB                                      &       Talal             \\
 \hline
* User Interface                                                  &           Darshan         \\
 \hline
* Backend                                                         &         Vinoth           \\
 \hline
* Frontend                                                        &    Darshan                \\
 \hline
* Testing \& Deployment  &  Gerrit         \\
 \hline
3. Benchmarks                                                     &                    \\
 \hline
* Introduction - Motivation                                             & Ron \\
 \hline
* Benchmark Overhead (Data Volume)      & Gerrit \\  
 \hline
* Benchmark Overhead (Latency)      & Talal \\  
 \hline
* Failure Benchmark                                             &       Vinoth     \\
 \hline
4. Conclusion                                                     &     Kevin               \\
 \hline
\end{tabular}
\end{center}
\caption{Authors of Sections}
\label{table:authors}
\end{table}


\section{Use Case}

A smart grid is an electrical grid which includes a variety of operational and energy measures including smart meters, smart appliances, renewable energy resources, and energy efficient resources.
Electronic power conditioning and control of the production and distribution of electricity are important aspects of the smart grid. 
Providing reliability in smart grids are done using electronic control, metering, and monitoring. 
To motivate our design decisions we are looking at how a potential user would interact with our system. 
In this chapter we are looking at the Administrators of a small to medium sized smart grid. 

Their main goals are to find failures in near-realtime and to be able to analyze these failures manually. 
Furthermore they want to have data available for offline analysis to compare overall and individual component performances over time. 
The components that are most relevant for this purpose are the gateway nodes that relay and potentially alter the messages produced by the sensors.

\vspace{3mm}

The system administrators have various tools available for monitoring their system. 
To detect failures heartbeat messages can be implemented, to trace latencies of messages existing tracing systems can be deployed and debugging and logging tools can be used to capture potentially relevant information for manual as well as automatic analysis.
However, a combined and more specialized solution has the potential to provide more value.

\subsection{Problems}

The administrators are facing the following main challenges:

\begin{itemize}
  \item Information on node health should be available as fast as possible.
  \item Data for debugging should be available at one location independent of the grid.
  \item Not all Members of the team have the same computer science Background. A simplified interface for manual tasks is required.
  \item Hardware Resources on the Grid are limited. 
  \item The administrators need to know, how much Overhead additional monitoring tools will introduce on the gateways.
\end{itemize}

%\subsubsection{Value Proposition}

%Here is an overview of the features our system provides. Each will be explained in more detail in the following Example section.

%\begin{itemize}
%  \item Integrated solution: Only the gathering of context information has to be implemented on the pipeline level, then the provenance components will take care of data transfer and storage.
%  \item Node Health Monitoring through heartbeat messages and send/receive rates.
%  \item Direct queries as well as an simplified interface are available.
%\end{itemize}

\subsection{Example Workflows}

The following examples are written as user stories and intended to define what the system administrator team is expecting from the system.

\subsubsection{Installation}
\begin{itemize}
  \item The provenance daemon has to be installed on all nodes that are to be tracked
  \item For context parameters such as "Line of Code" the existing code running on the gateways needs to be altered to expose such information via the provenance api.
  \item the Database, Backend and Frontend Services can be installed on any server
\end{itemize}

\subsubsection{Monitoring}
\begin{itemize}
  \item The user has a visual overview of all nodes that are part of the grid.
  \item The overview provides information on node health based on heartbeat messages and send/receive rates
  \item Changes in a node's health are signaled through changes in colors:
	\begin{itemize}
	  \item Green: Good health is inferred from recent messages.
	  \item Yellow: Possible problem when node takes longer to respond to messages (Default 5 sec)
	  \item Red: A failure is confirmed or the node has not responded after >10 sec.
	\end{itemize}
\end{itemize}

\subsubsection{Failure Scenarios}
\begin{itemize}
  \item Node Failure: If a node does not react to messages from its neighbours, including heartbeat messages, it is assumed that the whole node has failed. 
  \item Channel Failure: If heartbeat messages reveal that two nodes are healthy, but messages are not delivered between them, a problem with the network link can be inferred.
  \item Pipeline Daemon: If the process responsible for relaying sensor messages is unresponsive, this information is propagated through heartbeat messages.
  \item Provenance Daemon: If the process responsible for capturing and sending the provenance information is unresponsive, it is captured in the next heartbeat message.
\end{itemize}

\subsubsection{Analysis}
\begin{itemize}
  \item Through the UI the user can click on a node to see messages that have passed recently.
  \item for each message, provenance data is visible in the UI or as a JSON download.
  \item when looking for specific information the user can also use the UI query tool (e.g. find message based on ID).
  \item when a failure occurs the user can look at the information of the last messages that passed 
  \item for larger queries like when comparing system wide performance for a given time period, the database should be queried directly.
\end{itemize}

Once an issue is identified, it is assumed that the users will use their own tools and possibly physical access to solve them.

